{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5326fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio\n",
    "from tqdm import trange  # progress bar\n",
    "\n",
    "# -----------------------\n",
    "# 1. Problem & Cost\n",
    "# -----------------------\n",
    "p, a_cost, b_cost = 1, 1, 2\n",
    "def inventory_cost(s_next, a):\n",
    "    order_cost = p * a\n",
    "    inv_cost = a_cost * s_next if s_next >= 0 else b_cost * (-s_next)\n",
    "    return order_cost + inv_cost\n",
    "\n",
    "# -----------------------\n",
    "# 2. Actor–Critic Model\n",
    "# -----------------------\n",
    "class ActorCritic(nn.Module):\n",
    "    def _init_(self, state_dim, num_actions):\n",
    "        super()._init_()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64),         nn.ReLU(),\n",
    "            nn.Linear(64, num_actions)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64),         nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.actor(x)\n",
    "        value  = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "    def get_action(self, state):\n",
    "        logits, value = self.forward(state)\n",
    "        dist   = Categorical(logits=logits)\n",
    "        a      = dist.sample()\n",
    "        logp   = dist.log_prob(a)\n",
    "        return a.item(), logp, value.squeeze()\n",
    "\n",
    "# -----------------------\n",
    "# 3. Hyperparameters\n",
    "# -----------------------\n",
    "num_actions  = 10\n",
    "T            = 100\n",
    "min_inv      = -100\n",
    "max_inv      = 100\n",
    "state_dim    = 2\n",
    "gamma        = 1.0\n",
    "lr           = 1e-3\n",
    "num_episodes = 2000\n",
    "\n",
    "# -----------------------\n",
    "# 4. Setup\n",
    "# -----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent  = ActorCritic(state_dim, num_actions).to(device)\n",
    "opt    = optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "# -----------------------\n",
    "# 5. Training Loop\n",
    "# -----------------------\n",
    "for ep in trange(num_episodes, desc=\"Training\"):\n",
    "    # start from a random inventory level\n",
    "    s = random.randint(min_inv, max_inv)\n",
    "    for t in range(T):\n",
    "        # normalize state into [–1,1]×[0,1]\n",
    "        st      = torch.tensor([s/max_inv, t/(T-1)],\n",
    "                               dtype=torch.float32, device=device)\n",
    "        a, logp, v = agent.get_action(st)\n",
    "\n",
    "        # step in environment\n",
    "        w   = random.randint(0, 10)\n",
    "        s2  = max(min_inv, min(max_inv, s + a - w))\n",
    "        cost= inventory_cost(s2, a)\n",
    "\n",
    "        # estimate value of next state\n",
    "        with torch.no_grad():\n",
    "            t2   = 0 if t == T-1 else (t+1)\n",
    "            st2  = torch.tensor([s2/max_inv, t2/(T-1)],\n",
    "                                dtype=torch.float32, device=device)\n",
    "            _, v2 = agent.forward(st2)\n",
    "            v2     = v2.squeeze()\n",
    "\n",
    "        done_mask = float(t == T-1)\n",
    "        # advantage = (immediate cost + γ·V(next)) – V(current)\n",
    "        advantage = cost + gamma * v2 * (1 - done_mask) - v\n",
    "\n",
    "        # actor: increase log-prob of actions that lower cost\n",
    "        actor_loss  = logp * advantage.detach()\n",
    "        critic_loss = advantage.pow(2)\n",
    "        loss        = actor_loss + critic_loss\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        s = s2\n",
    "\n",
    "# -----------------------\n",
    "# 6. Build Policy & Value Tables\n",
    "# -----------------------\n",
    "agent.eval()\n",
    "num_states   = max_inv - min_inv + 1\n",
    "policy_table = np.zeros((num_states, T), dtype=int)\n",
    "V_table      = np.zeros((num_states, T), dtype=float)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, s in enumerate(range(min_inv, max_inv+1)):\n",
    "        for t in range(T):\n",
    "            st     = torch.tensor([[s/max_inv, t/(T-1)]],\n",
    "                                  dtype=torch.float32, device=device)\n",
    "            logits, v = agent.forward(st)\n",
    "            # pick cheapest action = smallest logit\n",
    "            policy_table[idx, t] = int(logits.argmin(dim=-1).item())\n",
    "            V_table[idx, t]      = v.item()\n",
    "\n",
    "# -------------------------\n",
    "# 7. Compute Base-Stock σ_t\n",
    "# -------------------------\n",
    "sigma = []\n",
    "for t in range(T):\n",
    "    row = policy_table[:, t]\n",
    "    th  = max_inv\n",
    "    for i, a in enumerate(row):\n",
    "        s_val = i + min_inv\n",
    "        if a == 0:        # action 0 = “no order”\n",
    "            th = s_val\n",
    "            break\n",
    "    sigma.append(th)\n",
    "\n",
    "print(\"Period | σ_t\")\n",
    "print(\"---------------\")\n",
    "for t, z in enumerate(sigma):\n",
    "    print(f\"{t:6d} | {z:4d}\")\n",
    "\n",
    "# -----------------------\n",
    "# 8. Simulate & Print One Rollout\n",
    "# -----------------------\n",
    "print(\"\\nTime | Inv | σ_t | Demand | Action\")\n",
    "print(\"------------------------------------\")\n",
    "s = 0\n",
    "for t in range(T):\n",
    "    dem = random.randint(0, 10)\n",
    "    a   = policy_table[s - min_inv, t]\n",
    "    print(f\"{t:4d} | {s:4d} | {sigma[t]:4d} | {dem:6d} | {a:6d}\")\n",
    "    s = max(min_inv, min(max_inv, s + a - dem))\n",
    "\n",
    "# -----------------------\n",
    "# 9. Plot V(s) Evolution as GIF\n",
    "# -----------------------\n",
    "os.makedirs('frames', exist_ok=True)\n",
    "frames = []\n",
    "for t in range(T):\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.plot(range(min_inv, max_inv+1),\n",
    "             V_table[:, t], marker='o')\n",
    "    plt.title(f\"V(s) at t={t}\")\n",
    "    plt.xlabel(\"Inventory s\")\n",
    "    plt.ylabel(\"V(s,t)\")\n",
    "    plt.grid(True)\n",
    "    fn = f'frames/frame_{t:03d}.png'\n",
    "    plt.savefig(fn)\n",
    "    plt.close()\n",
    "    frames.append(fn)\n",
    "\n",
    "with imageio.get_writer('vf_evolution_ac_2.gif', mode='I', duration=0.1) as writer:\n",
    "    for fn in frames:\n",
    "        writer.append_data(imageio.v2.imread(fn))\n",
    "\n",
    "# clean up temp frames\n",
    "import shutil\n",
    "shutil.rmtree('frames')\n",
    "\n",
    "print(\"GIF saved as 'vf_evolution_ac_2.gif'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
