{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE version 2 with support added for extensions to problem statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_dims, action_dims, hidden_size=64, max_order=[10.0]):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dims, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mean = nn.Linear(hidden_size, action_dims)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dims))\n",
    "        self.max_order = np.array(max_order, dtype=np.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)             # now [1, state_dim]\n",
    "            squeeze_output = True\n",
    "        else:\n",
    "            squeeze_output = False\n",
    "\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        mu = self.mean(h)\n",
    "        std = torch.exp(self.log_std).clamp(1e-3, 2.0)\n",
    "\n",
    "        # if we unsqueezed, squeeze outputs back to 1D\n",
    "        if squeeze_output:\n",
    "            mu = mu.squeeze(0)\n",
    "            std = std.squeeze(0)\n",
    "\n",
    "        return mu, std\n",
    "\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        mu, std = self(state)\n",
    "        dist = Normal(mu, std)\n",
    "        raw = dist.rsample()\n",
    "        a = torch.tanh(raw) * (self.max_order / 2) + (self.max_order / 2)\n",
    "        logp = dist.log_prob(raw).sum(-1) - torch.log((self.max_order/2)*(1 - torch.tanh(raw).pow(2)) + 1e-6).sum(-1)\n",
    "        return a, logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InventoryEnv(gym.Env):\n",
    "    def __init__(self, min_inv=[-10], max_inv=[10], max_order=[10], horizon=100, price=[1.0], a_cost=[1.0], b_cost=[2.0]):\n",
    "        super().__init__()\n",
    "        self.min_inv, self.max_inv = np.array(min_inv, dtype=np.float32), np.array(max_inv, dtype=np.float32)\n",
    "        self.max_order = np.array(max_order, dtype=np.float32)\n",
    "        self.horizon = horizon\n",
    "        self.p, self.a_cost, self.b_cost = np.array(price, dtype=np.float32), np.array(a_cost, dtype=np.float32), np.array(b_cost, dtype=np.float32)\n",
    "    \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array(np.concatenate([min_inv, [0]]), dtype=np.float32),\n",
    "            high=np.array(np.concatenate([max_inv, [horizon]]), dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        print(self.observation_space)\n",
    "\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.zeros(len(self.max_order), dtype=np.float32),\n",
    "            high=np.array(max_order, dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        l = []\n",
    "        for i in range(len(self.min_inv)): \n",
    "            l.append(np.random.randint(self.min_inv[i], self.max_inv[i] + 1))\n",
    "        self.s = np.array(l)\n",
    "        self.t = 0\n",
    "        return np.array(l.append(self.t), dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 1. ensure numpy array and enforce bounds [0, max_order] per good\n",
    "        a = np.clip(np.array(action, dtype=np.float32), 0, self.max_order)\n",
    "\n",
    "        # 2. sample a continuous demand for each good\n",
    "        w = np.random.uniform(0, 10, size=self.s.shape)\n",
    "\n",
    "        # 3. compute next inventory before and after clamping\n",
    "        s_next = self.s + a - w\n",
    "        s_next_clamped = np.clip(s_next, self.min_inv, self.max_inv)\n",
    "\n",
    "        # 4. update state\n",
    "        self.s = s_next_clamped\n",
    "        self.t += 1\n",
    "\n",
    "        # 5. compute cost:\n",
    "        #    ordering cost: p * a\n",
    "        #    holding cost if inventory â‰¥ 0: a_cost * s_next_clamped\n",
    "        #    backorder cost if inventory < 0: b_cost * (-s_next_clamped)\n",
    "        ordering_cost = self.p * a\n",
    "        holding_cost  = self.a_cost * np.maximum(s_next_clamped, 0)\n",
    "        backorder_cost= self.b_cost * np.maximum(-s_next_clamped, 0)\n",
    "        total_cost = np.sum(ordering_cost + holding_cost + backorder_cost)\n",
    "\n",
    "        reward = -total_cost\n",
    "\n",
    "        # 6. done flag\n",
    "        done = (self.t >= self.horizon)\n",
    "\n",
    "        # 7. construct observation = [inventory_vector, time]\n",
    "        obs = np.concatenate([self.s, [self.t]]).astype(np.float32)\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
