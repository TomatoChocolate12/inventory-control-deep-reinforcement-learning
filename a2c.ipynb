{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d15f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, imageio, shutil\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "# -----------------------\n",
    "# 1) ENVIRONMENT\n",
    "# -----------------------\n",
    "class InventoryEnv:\n",
    "    def __init__(self, min_inv=-50, max_inv=50, T=50):\n",
    "        self.min_inv = min_inv\n",
    "        self.max_inv = max_inv\n",
    "        self.T       = T\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.s = random.randint(self.min_inv, self.max_inv)\n",
    "        self.t = 0\n",
    "        return np.array([self.s, self.t], dtype=np.float32)\n",
    "\n",
    "    def step(self, a):\n",
    "        # w ~ uniform{0..10}\n",
    "        w  = random.randint(0, 10)\n",
    "        s2 = self.s + a - w\n",
    "        s2 = max(self.min_inv, min(self.max_inv, s2))\n",
    "        # cost\n",
    "        p, a_cost, b_cost = 1, 1, 2\n",
    "        order_cost = p * a\n",
    "        inv_cost   = a_cost * s2 if s2 >= 0 else b_cost * (-s2)\n",
    "        cost       = order_cost + inv_cost\n",
    "        # reward = negative cost\n",
    "        r = -cost\n",
    "        # time\n",
    "        self.t += 1\n",
    "        done = (self.t >= self.T)\n",
    "        self.s = s2\n",
    "        obs2 = np.array([self.s, self.t if not done else 0], dtype=np.float32)\n",
    "        return obs2, r, done\n",
    "\n",
    "# -----------------------\n",
    "# 2) A2C MODEL\n",
    "# -----------------------\n",
    "class A2C(nn.Module):\n",
    "    def __init__(self, state_dim, num_actions):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64),        nn.ReLU(),\n",
    "        )\n",
    "        self.actor  = nn.Linear(64, num_actions)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.shared(x)\n",
    "        return self.actor(h), self.critic(h)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        logits, value = self.forward(state)\n",
    "        dist  = torch.distributions.Categorical(logits=logits)\n",
    "        a     = dist.sample()\n",
    "        logp  = dist.log_prob(a)\n",
    "        return a.item(), logp, value.squeeze()\n",
    "\n",
    "# -----------------------\n",
    "# 3) HYPERPARAMETERS\n",
    "# -----------------------\n",
    "num_actions   = 10\n",
    "T             = 50\n",
    "min_inv, max_inv = -50, 50\n",
    "state_dim     = 2\n",
    "gamma         = 1.0\n",
    "lr            = 1e-3\n",
    "episodes      = 50000\n",
    "critic_coef   = 0.5\n",
    "entropy_coef  = 0.01\n",
    "\n",
    "# -----------------------\n",
    "# 4) SETUP\n",
    "# -----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env    = InventoryEnv(min_inv, max_inv, T)\n",
    "agent  = A2C(state_dim, num_actions).to(device)\n",
    "opt    = optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "# for convergence monitoring\n",
    "adv_history = []\n",
    "\n",
    "# -----------------------\n",
    "# 5) TRAIN\n",
    "# -----------------------\n",
    "for ep in trange(episodes, desc=\"Training\"):\n",
    "    # storage\n",
    "    logps, values, rewards = [], [], []\n",
    "    # init\n",
    "    obs = env.reset()\n",
    "    for t in range(T):\n",
    "        # normalize state: s/max_inv∈[–1,1], t/(T-1)∈[0,1]\n",
    "        st = torch.tensor([obs[0]/max_inv, obs[1]/(T-1)],\n",
    "                          dtype=torch.float32, device=device)\n",
    "        a, logp, v = agent.get_action(st)\n",
    "        obs2, r, done = env.step(a)\n",
    "        logps .append(logp)\n",
    "        values.append(v)\n",
    "        rewards.append(r)\n",
    "        obs = obs2\n",
    "        if done: break\n",
    "\n",
    "    # compute returns & advantages\n",
    "    returns = []\n",
    "    G = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "    values  = torch.stack(values)\n",
    "    logps   = torch.stack(logps)\n",
    "    advs    = returns - values\n",
    "    adv_history.append(advs.abs().mean().item())\n",
    "\n",
    "    # losses\n",
    "    actor_loss  = -(logps * advs.detach()).mean()\n",
    "    critic_loss = advs.pow(2).mean()\n",
    "    # entropy bonus\n",
    "    # (optional) encourage exploration\n",
    "    # ent = torch.distributions.Categorical(logits=agent.forward(st)[0]).entropy().mean()\n",
    "    # loss = actor_loss + critic_coef*critic_loss - entropy_coef*ent\n",
    "    loss = actor_loss + critic_coef*critic_loss\n",
    "\n",
    "    # backward\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "# -----------------------\n",
    "# 6) PLOT ADVANTAGE CONVERGENCE\n",
    "# -----------------------\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(adv_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Mean |Advantage|\")\n",
    "plt.title(\"Critic Convergence (Advantage → 0)\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# 7) EXTRACT POLICY & V(s,t)\n",
    "# -----------------------\n",
    "agent.eval()\n",
    "num_states = max_inv - min_inv + 1\n",
    "pi_tab     = np.zeros((T, num_states), dtype=int)\n",
    "V_tab      = np.zeros((T, num_states), dtype=float)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in range(T):\n",
    "        for i, s in enumerate(range(min_inv, max_inv+1)):\n",
    "            st = torch.tensor([[s/max_inv, t/(T-1)]],\n",
    "                              dtype=torch.float32, device=device)\n",
    "            logits, v = agent.forward(st)\n",
    "            pi_tab[t,i] = int(logits.argmin(dim=-1).item())\n",
    "            V_tab [t,i] = v.item()\n",
    "\n",
    "# -----------------------\n",
    "# 8) COMPUTE BASE‐STOCK σ_t\n",
    "# -----------------------\n",
    "sigma = []\n",
    "for t in range(T):\n",
    "    th = max_inv\n",
    "    for i,a in enumerate(pi_tab[t]):\n",
    "        s = i + min_inv\n",
    "        if a == 0:\n",
    "            th = s\n",
    "            break\n",
    "    sigma.append(th)\n",
    "\n",
    "print(\"Period | σ_t\")\n",
    "print(\"---------------\")\n",
    "for t,z in enumerate(sigma):\n",
    "    print(f\"{t:6d} | {z:4d}\")\n",
    "\n",
    "# -----------------------\n",
    "# 9) ROLLOUT DEMO\n",
    "# -----------------------\n",
    "print(\"\\nTime | Inv | σ_t | Demand | Action\")\n",
    "print(\"------------------------------------\")\n",
    "s = 0\n",
    "for t in range(T):\n",
    "    w = random.randint(0,10)\n",
    "    a = pi_tab[t, s-min_inv]\n",
    "    print(f\"{t:4d} | {s:4d} | {sigma[t]:4d} | {w:6d} | {a:6d}\")\n",
    "    s = max(min_inv, min(max_inv, s + a - w))\n",
    "\n",
    "# -----------------------\n",
    "# 10) GIF OF V(s) EVOLUTION\n",
    "# -----------------------\n",
    "os.makedirs(\"frames\", exist_ok=True)\n",
    "states = np.arange(min_inv, max_inv+1)\n",
    "for t in range(T):\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.plot(states, V_tab[t], marker='o')\n",
    "    plt.title(f\"V(s) at t={t}\")\n",
    "    plt.xlabel(\"Inventory\")\n",
    "    plt.ylabel(\"V(s,t)\")\n",
    "    plt.grid(True)\n",
    "    fn = f\"frames/{t:03d}.png\"\n",
    "    plt.savefig(fn); plt.close()\n",
    "    imageio.imwrite(fn, imageio.imread(fn))\n",
    "\n",
    "with imageio.get_writer(\"vf_evolution_a2c.gif\", mode=\"I\", duration=0.1) as writer:\n",
    "    for t in range(T):\n",
    "        writer.append_data(imageio.imread(f\"frames/{t:03d}.png\"))\n",
    "\n",
    "shutil.rmtree(\"frames\")\n",
    "print(\"GIF saved as vf_evolution_a2c.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ebfeed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
